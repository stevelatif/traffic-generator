* Traffic Generation With SMBClient and Macvlans
:PROPERTIES:
:ID:       5354aa94-4145-4d35-92f1-c1d6de4c4acb
:END:
Many years ago I had to replicate a customer issue of a  1000+ windows
clients mounting a shared drive. With some luck, a few band aids some
C and Perl and custom hardware I was able to build something that
worked some of the time and allowed us to replicate the scenario in
the lab.

The scenario that we were trying to replicate was something like:
- A large lab with a single Windows Filer
- A thousand client machines that would mount a shared drive when the staff 
  started work in the morning.

 The final setup to emulate this was a single Linux box running:
 -  [[https://linux.die.net/man/1/smbclient][smbclient]] CLI tool to generate the traffic
 - threaded C code to drive the client and
 - system calls [[https://man7.org/linux/man-pages/man3/dlopen.3.html][dlopen]] and [[https://man7.org/linux/man-pages/man2/bind.2.html][bind]] to simulate a thousand connections by binding to
   [[https://docs.kernel.org/networking/alias.html][IP aliased interfaces]]
 - network emulator [[https://www.iwl.com/products/maxwell-pro][Maxwell]] from the good people at [[https://www.iwl.com/][IWL]] to rewrite the ethernet
   headers.

  The following diagram is an illustration of the set up 
[[./images/maxwell.png]]

A standard network client socket works in the following way:
[[./images/client.png]]

To get the client socket to be associated with the a particular IP address we
can use the [[https://man7.org/linux/man-pages/man2/bind.2.html][bind]] system call. This introduces a problem, the socket call is
being made by the smbclient program. We could get the source code and alter it
to make the bind call. We would also need to pass some extra parameters to tell
it which interface or address to bind to. The codebase is large and the
complexities are starting to mount.

Instead we can do a `hack`, if we can load
our own version of the library with the socket system call we can create a
socket in the usual way, get the file descriptor that is returned and then use
that to bind the socket to the interface we want. This is where LD_PRELOAD and
dl_sym come in.

[[./images/clientbind.png]]


The one remaining problem is that the IP aliased interfaces only have unique IP
addresses, but the underlying MAC address is that of the actual physical
interface.

At the time we got around this problem by rewriting the Ethernet headers and
mapping the ethernet address to one based on the IP address for the packets
going out to the SMB filer. On the way back they were all rewritten to the
correct address.


The reason for the maxwell in this setup is that the IP aliased interfaces have
unique IP addresses, but they all have the same IP address as the underlying 

In the time since I built this there's been new features added to the Linux
networking stack. This article goes through some of the ideas that I tried to
get a more robust version working. The solution turned out to be more  
straight forward than the original implementation.

My initial thought was to do something similar to the original version.
- intercept the socket call using [[https://man7.org/linux/man-pages/man8/ld.so.8.html][LD_PRELOAD]]
- bind the socket to a local virtual interface
- rewrite the ethernet header using eBPF

 Instead of using C/Perl/C++, use Rust with
 - [[https://aya-rs.dev/][Aya]] for the eBPF bindings
 - [[https://github.com/geofft/redhook][Redhook]] for doing LD_PRELOAD

 After some work it was possible to get it to work without needing
  eBPF or LD_PRELOAD, only using Linux namespaces. At some point I might
  integrate eBPF back in for traffic shaping. 

  
** Side note on Linux virtual networking

For a synopsis on [[https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#][Linux Virtual Networking]]
We will be using the following

*** Namespaces
Partition of kernel resources, so processes in different namespaces see
different resources. 

*** Macvlans
Behaves like a virtual L2 interface on top of another one. So you can have
mutiple virtual interfaces on top of a physical interface. The limitation is
that the virtual interfaces cannot directly communicate with the host.

*** Bridge
Behaves like a network switch, forwards packets between interfaces

*** Veths
Behaves like a network cable - created in pairs. Packets transmitted on
one device on the pair are available on the other device


* Configuration
The idea is:
-  setup multiple macvlans
- Place each macvlan in a seperate namespace
- Connect the macvlans to the same parent interface in bridge mode
- run the traffic generation tool in a namespace

  [[./images/namespace.png]]

I implemented this in Rust, but to understand the configuration lets look at
the configuration in bash:

Here the parent interface is:
#+begin_src sh
ip a
3: enp16s0f0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 1c:86:0b:28:ec:7c brd ff:ff:ff:ff:ff:ff
    inet6 fe80::773b:34b7:f2fc:aa7e/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
 #+end_src
 

Lets configure two  macvlans  on that interface
  #+begin_src sh
    #!/bin/bash

    set -e

    # Names
    NS0="myns0"
    NS1="myns1"

    MACVLAN0="macvlan0"
    MACVLAN1="macvlan1"

    PARENT_IF="enp16s0f0"
    MACVLAN_IP0="192.168.56.10/24"
    MACVLAN_IP1="192.168.56.11/24"    

    # Clean up if re-running
    # ip netns del $NS 2>/dev/null || true
    #ip link del $MACVLAN 2>/dev/null || true

    # Create namespace
    ip netns add $NS0
    ip netns add $NS1    

    # Create macvlan in bridge mode
    ip link add $MACVLAN0 link $PARENT_IF type macvlan mode bridge
    ip link set $MACVLAN0 netns $NS0

    ip link add $MACVLAN1 link $PARENT_IF type macvlan mode bridge
    ip link set $MACVLAN1 netns $NS1

    # Assign IPs to macvlans and bring them up
    ip netns exec $NS0 ip addr add $MACVLAN_IP0 dev $MACVLAN0
    ip netns exec $NS0 ip link set $MACVLAN0 up
    ip netns exec $NS0 ip link set lo up

    ip netns exec $NS1 ip addr add $MACVLAN_IP1 dev $MACVLAN1
    ip netns exec $NS1 ip link set $MACVLAN1 up
    ip netns exec $NS1 ip link set lo up

    # Set default route (optional, if accessing external network)
    ip netns exec $NS0 ip route add default dev $MACVLAN0
    ip netns exec $NS1 ip route add default dev $MACVLAN1

#+end_src

Verify that the macvlans came up, checking in the respective namespaces:
#+begin_src sh
  sudo ip netns exec myns0 ip a
  104: macvlan0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
  link/ether b6:16:ca:59:9b:4c brd ff:ff:ff:ff:ff:ff link-netnsid 0
  inet 192.168.56.10/24 scope global macvlan0
  valid_lft forever preferred_lft forever
  inet6 fe80::b416:caff:fe59:9b4c/64 scope link 
  valid_lft forever preferred_lft forever

   sudo ip netns exec myns1 ip a
   105: macvlan1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
   link/ether ca:26:70:ca:6d:c6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
   inet 192.168.56.11/24 scope global macvlan1
   valid_lft forever preferred_lft forever
   inet6 fe80::c826:70ff:feca:6dc6/64 scope link 
   valid_lft forever preferred_lft forever

#+end_src

I have a system connected via ethernet cable to this interface at 192.168.56.20.
We can verify that its working by running ping in the respective namespaces:

#+begin_src sh
  sudo ip netns exec myns0 ping 192.168.56.20
  PING 192.168.56.20 (192.168.56.20) 56(84) bytes of data.
  64 bytes from 192.168.56.20: icmp_seq=1 ttl=64 time=0.728 ms
  64 bytes from 192.168.56.20: icmp_seq=2 ttl=64 time=0.681 ms
  64 bytes from 192.168.56.20: icmp_seq=3 ttl=64 time=0.689 ms
  64 bytes from 192.168.56.20: icmp_seq=4 ttl=64 time=0.692 ms
  --- 192.168.56.20 ping statistics ---
  4 packets transmitted, 4 received, 0% packet loss, time 3051ms
  rtt min/avg/max/mdev = 0.681/0.697/0.728/0.018 ms

  steve@tabriz:~/test$ sudo ip netns exec myns1 ping 192.168.56.20
  PING 192.168.56.20 (192.168.56.20) 56(84) bytes of data.
  64 bytes from 192.168.56.20: icmp_seq=1 ttl=64 time=0.665 ms
  64 bytes from 192.168.56.20: icmp_seq=2 ttl=64 time=0.351 ms
  64 bytes from 192.168.56.20: icmp_seq=3 ttl=64 time=0.692 ms
  64 bytes from 192.168.56.20: icmp_seq=4 ttl=64 time=0.696 ms
  --- 192.168.56.20 ping statistics ---
  4 packets transmitted, 4 received, 0% packet loss, time 3105ms
  rtt min/avg/max/mdev = 0.351/0.601/0.696/0.144 ms
#+end_src

Then we can check whats happening using wireshark running on the host that was
being pinged:

[[./images/pcap001.png]]

[[./images/pcap002.png]]

Comparing the two images we can see that the packets from 192.168.56.10 and
192.168.56.11 are coming from different MAC addresses:
- b6:16:ca:59:9b:4c
- ca:26:70:ca:6d:c6

Clean up 
#+begin_src sh
  # Clean up
  # Clean up
  ip netns del $NS0
  ip netns del $NS1  

#+end_src

** Interim Summary
So by just using the shell we have prototyped a setup where from a single host we
can send packets that appear to be coming from an arbitrary number of hosts .
The next step is to use this idea to build a program that will set up the
configuration and then run a specified number of connections.
The original spec called for at least one thousand unique connections to be run
concurrently. This would be possible using the shell, but will be easier to
manage with a more fully featured language. 

I chose Rust, but any other language like Python or golang would work,
the bash would start as a good jumping off point. 

** Rust Implementation
:PROPERTIES:
:ID:       d111dc63-6542-4284-9105-7d324c8d832e
:END:
Implementing in Rust allows us to use Rust's Tokio runtime to spawn huge numbers
of smbclient instances. We can also use crates that make working with large
numbers of network connections easier.

#+begin_src rust -n  
#[derive(Debug, Clone)]
struct LocalConfig {
    hosts: Ipv4AddrRange,
    count: usize,
    interface: String,
    cidr_suffix: String,
    base_namespace: String,
}

async fn spawn_task(config: LocalConfig, smb_address: Ipv4Addr, filename: &String) {
    let (tx, rx) = flume::bounded(10);

    for (idx, _ii) in config.hosts.enumerate() {
        let tx = tx.clone();
        let namespace_ii = format!("{}{}", config.base_namespace, idx);
        let add = format!("//{}/public/", smb_address);
        let ff = format!("get {}", filename);

        // Convert address string to Ipv4Addr
        task::spawn(async move {
            let output = Command::new("/usr/sbin/ip")
                .arg("netns")
                .arg("exec")
                .arg(namespace_ii)
                .arg("smbclient")
                .arg("-Uguest")
                .arg("-N")
                .arg(add)
                .arg(smb_address.to_string())
                .arg("-c")
                .arg(ff)
                //.env("LD_PRELOAD", "./libsocket_interceptor.so" )
                //.env("__CLIENT_ADDRESS__", &ii.to_string())
                .output()
                .await;

            match output {
                Ok(out) => {
                    println!(
                        "stdout: {:?}\n  stderr{:?}",
                        str::from_utf8(&out.stdout),
                        str::from_utf8(&out.stderr)
                    );
                    tx.send_async(0).await.unwrap();
                }
                Err(e) => {
                    eprintln!("could not format the command: {}", e);
                }
            }
        });
    }
    drop(tx);

    for ii in 0..config.count {
        let message = rx.recv().unwrap();
        println!("Task {ii} completed with output: {:?}", message);
    }
}
#+end_src

The LocalConfig struct at the top has a host fields of type [[https://docs.rs/ipnet/latest/ipnet/struct.Ipv4AddrRange.html][Ipv4AddrRange]]. An
iterator over a range of IPv4 addresses.

setup() creates the namespaces, the bridge and the macvlans.

The rest of the code should be easy to
understand from the shell example we started with. We use Tokio to spawn
concurrent instances of smbclient, each running in its own namespace

 Running this with two instances:
 #+begin_src sh
sudo ./traffic-runner -a 192.168.56.20 -f test_file.zero -i enp16s0f0 -n foo -b 192.168.56.30 -e 192.168.56.31  -c 24
Task 0 completed with output: 0
Task 1 completed with output: 0
deleting interface: macvlan0
deleting interface: macvlan1
 #+end_src

#+CAPTION: Packet Dump for two clients from the server
#+NAME:   fig:dump_003
[[./images/pcap003.png]]


 Running with 51 interfaces
 #+begin_src  sh
   sudo ./traffic-runner -a 192.168.56.20 -f test_file.zero -i enp16s0f0 -n foo -b 192.168.56.40 -e 192.168.56.91  -c 24
   Task 0 completed with output: 0 
   Task 1 completed with output: 0 
   Task 2 completed with output: 0
   ...
   Task 50 completed with output: 0
   Task 51 completed with output: 0
   deleting interface: macvlan0
   deleting interface: macvlan1
   ...
   deleting interface: macvlan50
   deleting interface: macvlan51

 #+end_src

#+CAPTION: Packet Dump for fifty one clients from the server
#+NAME:   fig:dump_004
[[./images/pcap004.png]]


At this point we have basic traffic generation coming from multiple clients. We
can add further tools to validate the files that are brought over using a
checksum. For now let's leave that as an exercise for the motivated reader or as
a possible further article.

Source Code: [[https://github.com/stevelatif/traffic-generator/tree/main/traffic-runner][Here]]
